Chapter 4 Spark SQL (Foundations)
---------------------------------
The RDD was the initial core programming abstraction when Spark was introduced to the world in 2012. In Spark
1.6, a new programming abstraction, called Structured APIs, was introduced. This is the preferred way of performing data processing for the majority of use cases. The Structured APIs were designed to enhance developers’ productivity with easy-to-use, intuitive, and expressive APIs.

The Spark SQL module consists of two main parts. The first one is the representation of the Structure APIs, called DataFrames and Datasets, that define the high-level APIs for working with structured data. The DataFrame concept was inspired by the Python pandas DataFrame; the main difference is that a DataFrame in Spark can handle a large volume of data that is spread across many machines. 

The second part of the Spark SQL module is the Catalyst optimizer, which does all the hard work behind the scenes to make your life easier and to speed up your data processing logic.It optimizes all the queries written in Spark SQL and DataFrame DSL. The optimizer helps us to run queries much faster than their counter RDD part. This increases the performance of the system. One of the cool features the Spark SQL module offers is the ability to execute SQL queries to perform data processing. By virtue of this capability, Spark is able to gain a new group of users called business analysts, who are familiar with the SQL language because it is one of the main tools they use on a regular basis.

One main concept that differentiates structured data from unstructured data is a schema, which defines the structure of the data in the form of column names and associated data types. The schema concept is an integral part of the Spark Structured APIs.

Structured data is often captured in a certain format. Some of the formats are text based, and some of them are binary based. Common formats for text data are CSV, XML,and JSON, and common formats for binary data are Avro, Parquet, and ORC

Out of the box, the Spark SQL module makes it easy to read data and write data from and to any of these formats. One unanticipated benefit comes out of this versatility is that Spark can be used as a data format conversion tool.

Introduction to DataFrames
--------------------------
A DataFrame is an immutable, distributed collection of data that is organized into rows, where each one consists a set of columns and each column has a name and an associated type. In other words, this distributed collection of data has a structure defined by a schema. 

Each row in the DataFrame is represented by a generic Row object. Unlike the RDD APIs, the DataFrame APIs offer a set of domain-specific operations that are relational and have rich semantics.

Like the RDD APIs, the DataFrame APIs are classified into two buckets: transformations and actions. The evaluation semantics are identical in RDDs. Transformations are lazily evaluated, and actions are eagerly evaluated.

DataFrames can be created by reading data from the many structured data sources mentioned previously as well as by reading data from tables in Hive and databases. In addition, the Spark SQL module makes it easy to convert an RDD to a DataFrame by providing the schema information about the data in the RDD. The DataFrame APIs are available in Scala, Java, Python, and R.

Creating DataFrames
-------------------
There are many ways to create a DataFrame; one common thing among them is the need to provide a schema, either implicitly or explicitly.

Creating DataFrames from RDDs
-----------------------------
Let’s start with creating a DataFrame from an RDD. Below code first creates an RDD with two columns of the integer type, and then it calls the toDF implicit function to convert an RDD to a DataFrame using the specified column names. The column types are inferred from the data in the RDD. Below code shows two of the commonly used functions in a DataFrame, printSchema and show. 

Function printSchema prints out the column names and their associated type to the console. Function show prints the data in a DataFrame out in a tabular format. By default, it displays 20 rows. To change the default number of rows to display, you can pass in a number to the show function. 


import scala.util.Random
val rdd = spark.sparkContext.parallelize(1 to 10).map(x => (x, Random.nextInt]+(100)* x))
val kvDF = rdd.toDF("key","value")

kvDF.printSchema
|-- key: integer (nullable = false)
|-- value: integer (nullable = false)

kvDF.show
+---+-----+
|key|value|
+---+-----+
| 1| 58|
| 2| 18|
| 3| 237|
| 4| 32|
| 5| 80|
| 6| 210|
| 7| 567|
| 8| 360|
| 9| 288|
| 10| 260|
+---+-----+

Calling the Function show to Display Five Rows in Tabular Format

kvDF.show(5)
+---+-----+
|key|value|
+---+-----+
| 1| 59|
| 2| 60|
| 3| 66|
| 4| 280|
| 5| 40|
+---+-----+

Another way to create a DataFrame is by specifying an RDD with a schema that is created programmatically.
first creates an RDD using an array of Row objects, where each Row object contains three columns; then it creates a schema programmatically. Finally, it provides the RDD and schema to the function createDataFrame to convert to a DataFrame. Listing 4-5 shows the schema and the data in the peopleDF DataFrame.

Creating a DataFrame from an RDD with a Schema Created Programmatically

import org.apache.spark.sql.Row
import org.apache.spark.sql.types._
val peopleRDD = spark.sparkContext.parallelize(Array(Row(1L, "John Doe", 30L),Row(2L, "Mary Jane", 25L)))
val schema = StructType(Array(
StructField("id", LongType, true),
StructField("name", StringType, true),
StructField("age", LongType, true)
))

val peopleDF = spark.createDataFrame(peopleRDD, schema)

peopleDF.printSchema
|-- id: long (nullable = true)
|-- name: string (nullable = true)
|-- age: long (nullable = true)

peopleDF.show
+--+-----------+---+
|id| name|age|
+--+-----------+---+
| 1| John Doe| 30|
| 2| Mary Jane| 25|
+--+-----------+---+

Each StructField object has three pieces of information: name, type, and whether the value is nullable.

The type of each column in a DataFrame is mapped to an internal Spark type, which can be a simple scalar type or a complex type. Table 4-1 describes the available internal Spark data types and associated Scala types.

Table 4-1. Spark Scala Types
Data Type 		Scala Type

BooleanType 		Boolean
ByteType 		Byte
ShortType 		Short
IntegerType 		Int
LongType 		Long
FloatType 		Float
DoubleType 		Double
DecimalType 		java.math.BigDecial
StringType 		String
BinaryType 		Array[Byte]
TimestampType 		java.sql.Timestamp
DateType 		java.sql.Date
ArrayType 		scala.collection.Seq
MapType 		scala.collection.Map
StructType 		org.apache.spark.sql.Row

Creating DataFrames from a Range of Numbers
-------------------------------------------
Spark 2.0 introduced a new entry point for Spark applications. It is represented by a class called SparkSession, which has a convenient function called range that can easily create a DataFrame with a single column with the name id and the type LongType. 

This function has a few variations that can take additional parameters to specify the start and end of a range as well as the steps of the range

Using the SparkSession.range Function to Create a DataFrame

val df1 = spark.range(5).toDF("num").show
+---+
|num|
+---+
| 0|
| 1|
| 2|
| 3|
| 4|
+---+
spark.range(5,10).toDF("num").show
+---+
|num|
+---+
| 5|
| 6|
| 7|
| 8|
| 9|
+---+
spark.range(5,15,2).toDF("num").show
+---+
|num|
+---+
| 5|
| 7|
| 9|
| 11|
| 13|
+---+

Notice the range function can create only a singlecolumn DataFrame. Do you have any ideas about how to create a DataFrame with more than one column?

One option to create a multicolumn DataFrame is to use Spark’s implicits that convert a collection of tuples inside a Scala Seq collection.

Converting a Collection Tuple to a DataFrame Using Spark’s toDF Implicit

val movies = Seq(("Damon, Matt", "The Bourne Ultimatum", 2007L),
		("Damon, Matt", "Good Will Hunting", 1997L))

val moviesDF = movies.toDF("actor", "title", "year")

moviesDF.printSchema
|-- actor: string (nullable = true)
|-- title: string (nullable = true)
|-- year: long (nullable = false)

moviesDF.show
+-----------+--------------------+----+
| actor| title|year|
+-----------+--------------------+----+
|Damon, Matt|The Bourne Ultimatum|2007|
|Damon, Matt| Good Will Hunting|1997|
+-----------+--------------------+----+

Creating DataFrames from Data Sources
-------------------------------------
Out of the box, Spark SQL supports six built-in data sources, where each data source is mapped to a data format.The data source layer in the Spark SQL module is designed to be extensible, so custom data sources can be easily integrated into the DataFrame APIs. There are hundreds of custom data sources written by the Spark community, and it is not too difficult to implement them.

The two main classes in Spark SQL for reading and writing data are DataFrameReader and DataFrameWriter, respectively.

Common Pattern for Interacting with DataFrameReader
spark.read.format(...).option("key", value").schema(...).load()

Below describes the three main pieces of information that are used when reading data: format, option, and schema.

Name 	Optional 	Comments
format  No 		This can be one of the built-in data sources or a custom format. For a built-in
			format, you can use a short name (json, parquet, jdbc, orc, csv, text).
			For a custom data source, you need to provide a fully qualified name. 

option  Yes 		DataFrameReader has a set of default options for each data source format.
			You can override those default values by providing a value to the option function.

schema Yes 		Some data sources have the schema embedded inside the data files, i.e.,
			Parquet and ORC. In those cases, the schema is automatically inferred. For
			other cases, you may need to provide a schema.

Specifying the Data Source Format

spark.read.json("<path>")
spark.read.format("json")

spark.read.parquet("<path>")
spark.read.format("parquet")

spark.read.jdbc
spark.read.format("jdbc")

spark.read.orc("<path>")
spark.read.format("orc")

spark.read.csv("<path>")
spark.read.format("csv")

spark.read.text("<path>")
spark.read.format("text")

// custom data source – fully qualifed package name
spark.read.format("org.example.mysource")

Please see the table in PDF

Creating DataFrames by Reading Text Files
-----------------------------------------
Text files contain unstructured data. As it is read into Spark, each line becomes a row in the DataFrame. For plain-text files, one common way to parse the words of each line is by splitting it with a space as a delimiter. This is similar to how a typical word count example works.

val textFile = spark.read.text("README.md")

textFile.printSchema
|-- value: string (nullable = true)

// show 5 lines and don't truncate
textFile.show(5, false)

+-------------------------------------------------------------------------+
|value |
+-------------------------------------------------------------------------+
|# Apache Spark |
| |
|Spark is a fast and general cluster computing system for Big Data. It provides |
|high-level APIs in Scala, Java, Python, and R, and an optimized engine that |
|supports general computation graphs for data analysis. It also supports a |
+-------------------------------------------------------------------------+

If a text file contains a delimiter that you can use to parse the columns in each line

Creating DataFrames by Reading CSV Files
----------------------------------------
The CSV parser in Spark is designed to be flexible such that it can parse a text file using a user-provided delimiter. The comma delimiter just happens to be the default one. This means you can use the CSV format to read tab-separated value text files or other text files with an arbitrary delimiter.

Some CSV files have a header, and some don’t. Since a column value may contain a comma, it is a good practice to escape it using a special character.

Please see the table in PDF for more CSV option

Specifying the header and inferSchema options as true won’t require you to specify a schema. Otherwise, you need to define a schema by hand or programmatically create it and pass it into the schema function. If the inferSchema option is false and no schema is provided, Spark will assume the data type of all the columns to be the string type.

The data file you are using as an example is called movies.csv in the folder data/. This file contains a header for each column: actor, title, year.

Reading CSV Files with Various Options

val movies = spark.read.option("header","true").csv("<path>/book/chapter4/data/movies/movies.csv")

movies.printSchema
|-- actor: string (nullable = true)
|-- title: string (nullable = true)
|-- year: string (nullable = true)

// now try to infer the schema
val movies2 = spark.read.option("header","true").option("inferSchema","true").csv("<path>/book/chapter4/data/movies/movies.csv")

movies2.printSchema
|-- actor: string (nullable = true)
|-- title: string (nullable = true)
|-- year: integer (nullable = true)

// now try to manually provide a schema
import org.apache.spark.sql.types._
val movieSchema = StructType(Array(StructField("actor_name", StringType, true),
StructField("movie_title", StringType, true),
StructField("produced_year", LongType, true)))

val movies3 = spark.read.option("header","true").schema(movieSchema).csv("<path>/book/chapter4/data/movies/movies.csv")

movies3.printSchema
|-- actor_name: string (nullable = true)
|-- movie_title: string (nullable = true)
|-- produced_year: long (nullable = true)

movies3.show(5)
+-----------------+--------------+--------------+
| actor_name| movie_title| produced_year|
+-----------------+--------------+--------------+
|McClure, Marc (I)| Freaky Friday| 2003|
|McClure, Marc (I)| Coach Carter| 2005|
|McClure, Marc (I)| Superman II| 1980|
|McClure, Marc (I)| Apollo 13| 1995|
|McClure, Marc (I)| Superman| 1978|
+-----------------+--------------+--------------+

Now let’s try to read in a text file with a delimiter that is different, not a comma. Instead, it is a tab. In this case, you specify a value for the sep option that Spark will use.

Reading a TSV File with the CSV Format
val movies4 = spark.read.option("header","true").option("sep", "\t").schema(movieSchema).csv("<path>/book/chapter4/data/movies/movies.tsv")

movies.printSchema
|-- actor_name: string (nullable = true)
|-- movie_title: string (nullable = true)
|-- produced_year: long (nullable = true)

Creating DataFrames by Reading JSON Files
-----------------------------------------
JSON is a well-known format in the JavaScript community. It is considered to be a semistructured format because each object (aka row) has a structure and each column
has a name.

One of the strengths of JSON is that it provides a flexible format that can model any use case; it can also support a nested structure. JSON has one disadvantage that is related to verbosity.The column names are repeated in each row in the data file.

Spark makes it easy to read data in a JSON file. However, there is one thing that you need to pay attention to. A JSON object can be expressed on a single line or across multiple lines, and this is something you need to let Spark know.

Given that a JSON data file contains only column names and no data type, how is Spark able to come up with a schema? Spark tries its best to infer the schema by parsing a set of sample records. The number of records to sample is determined by the samplingRatio option, which has a default value of 1.0. Therefore, it is quite expensive to read a large JSON file. In this case,you can lower the samplingRatio value to speed up the data loading process.

Below code shows two examples of reading JSON files. The first one simply reads a JSON file without overriding any option value. Notice Spark automatically detects
the column name and data type based on the information in the JSON file. The second example specifies a schema.

Various Examples of Reading a JSON File

val movies5 = spark.read.json("<path>/book/chapter4/data/movies/movies.json")

movies.printSchema
|-- actor_name: string (nullable = true)
|-- movie_title: string (nullable = true)
|-- produced_year: long (nullable = true)

// specify a schema to override the Spark's inferring schema.
// producted_year is specified as integer type
import org.apache.spark.sql.types._
val movieSchema2 = StructType(Array(StructField("actor_name", StringType, true),
StructField("movie_title", StringType, true),
StructField("produced_year", IntegerType, true)))

val movies6 = spark.read.option("inferSchema","true").schema(movieSchema2).json("<path>/book/chapter4/data/movies/movies.json")

movies6.printSchema
|-- actor_name: string (nullable = true)
|-- movie_title: string (nullable = true)
|-- produced_year: integer (nullable = true)

What happens when a column data type specified in the schema doesn’t match up with the value in the JSON file? By default, when Spark encounters a corrupted record or
runs into a parsing error, it will set the value of all the columns in that row to null. Instead of getting null values, you can tell Spark to fail fast.

Parsing Error and How to Tell Spark to Fail Fast
// set data type for actor_name as BooleanType
import org.apache.spark.sql.types._
val badMovieSchema = StructType(Array(StructField("actor_name",BooleanType, true),
StructField("movie_title",StringType, true),
StructField("produced_year",IntegerType, true)))

val movies7 = spark.read.schema(badMovieSchema). json("<path>/book/chapter4/data/movies/movies.json")

movies7.printSchema
|-- actor_name: boolean (nullable = true)
|-- movie_title: string (nullable = true)
|-- produced_year: integer (nullable = true)

movies7.show(5)
+----------+-----------+-------------+
|actor_name|movie_title|produced_year|
+----------+-----------+-------------+
| null| null| null|
| null| null| null|
| null| null| null|
| null| null| null|
| null| null| null|
+----------+-----------+-------------+

// tell Spark to fail fast when facing a parsing error
val movies8 = spark.read.option("mode","failFast").schema(badMovieSchema).json("<path>/book/chapter4/data/movies/movies.json")

movies8.printSchema
|-- actor_name: boolean (nullable = true)
|-- movie_title: string (nullable = true)
|-- produced_year: integer (nullable = true)

// Spark will throw a RuntimeException when executing an action
movies8.show(5)
ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)
java.lang.RuntimeException: Failed to parse a value for data type
BooleanType (current token: VALUE_STRING).

Creating DataFrames by Reading Parquet Files
--------------------------------------------
Parquet is one of the most popular open source columnar storage formats in the Hadoop ecosystem, and it was created at Twitter. Its popularity is because it is a self-describing data format and it stores data in a highly compact structure by leveraging compressions.

The columnar storage format is designed to work well with a data analytics workload where only a small subset of the columns are used during the data analysis. Parquet stores the data of each column in a separate file; therefore, columns that are not needed in a data analysis wouldn’t have to be unnecessarily read in. 

It is quite flexible when it comes to supporting a complex data type with a nested structure. Text file formats such as CVS and JSON are good for small files, and they are human-readable. For working with large datasets that are stored in long-term storage, Parquet is a much better file format to use to reduce storage costs and to speed up the reading step.

Spark works extremely well with the Parquet file format, and in fact Parquet is the default file format for reading and writing data in Spark. Since Parquet files are selfcontained, meaning the schema is stored inside the Parquet data file, it is easy to work with Parquet in Spark

Notice that you don’t need to provide a schema or ask Spark to infer the schema. Spark can retrieve the schema from the Parquet file.One of the cool optimizations that Spark does when reading data from Parquet is that it does decompression and decoding in column batches.

Reading a Parquet File in Spark
// Parquet is the default format, so we don't need to specify the format
when reading
val movies9 = spark.read.load("<path>/book/chapter4/data/movies/movies.parquet")

movies9.printSchema
|-- actor_name: string (nullable = true)
|-- movie_title: string (nullable = true)
|-- produced_year: long (nullable = true)

// If we want to be more explicit, we can specify the path to the parquet function
val movies10 = spark.read.parquet("<path>/book/chapter4/data/movies/movies.parquet")

movies10.printSchema
|-- actor_name: string (nullable = true)
|-- movie_title: string (nullable = true)
|-- produced_year: long (nullable = true)

Creating DataFrames by Reading ORC Files
----------------------------------------
Optimized Row Columnar (ORC) is another popular open source self-describing columnar storage format in the Hadoop ecosystem. It was created by a company called Cloudera as part of the initiative to massively speed up Hive. It is quite similar to Parquet in terms of efficiency and speed and was designed for analytics workloads. Working with ORC files is just as easy as working with Parquet files.

Reading an ORC File in Spark
val movies11 = spark.read.orc("<path>/book/chapter4/data/movies/movies.orc")

movies11.printSchema
|-- actor_name: string (nullable = true)
|-- movie_title: string (nullable = true)
|-- produced_year: long (nullable = true)

movies11.show(5)
+--------------------------+-------------------+--------------+
| actor_name| movie_title| produced_year|
+--------------------------+-------------------+--------------+
| McClure, Marc (I)| Coach Carter| 2005|
| McClure, Marc (I)| Superman II| 1980|
| McClure, Marc (I)| Apollo 13| 1995|
| McClure, Marc (I)| Superman| 1978|
| McClure, Marc (I)| Back to the Future| 1985|
+--------------------------+-------------------+--------------+

Creating DataFrames from JDBC
-----------------------------
JDBC is a standard application API for reading data from and writing data to a relational database management system. Spark has support for JDBC data sources, which means you can use Spark to read data from and write data to any of the existing RDBMSs such as MySQL, PostgreSQL, Oracle, SQLite, and so on. There are a few important pieces of information you need to provide when working with a JDBC data source: a JDBC driver for your RDBMS, a connection URL, authentication information, and a table name.

For Spark to connect to an RDBMS, it must have access to the JDBC driver JAR file at runtime. Therefore, you need to add the location of a JDBC driver to the Spark classpath.

You need to specify a JDBC Driver When Starting a Spark Shell, there is a command please see in PDF.

Once the Spark shell is successfully started, you can quickly verify to see whether Spark can connect to the RDBMS by using the java.sql.DriverManager class, as shown below. This example is trying to test a connection to MySQL. The URL format will be a bit different if your RDBMS is not MySQL, so consult the documentation of the JDBC driver you are using.

Testing the Connection to MySQL in the Spark Shell

import java.sql.DriverManager
val connectionURL = "jdbc:mysql://localhost:3306/<table>?user=<username>&password=<password>"
val connection = DriverManager.getConnection(connectionURL)
connection.isClosed()
connection close()

If you didn’t get any exception about the connection, then the Spark shell was able to successfully connect to your RDBMS.

Key 		Description
url 		The JDBC URL for Spark to connect to. At the minimum, it should contain the host, port,
		and database name. For MySQL, it may look something like this: 		jdbc:mysql://localhost:3306/sakila.

dbtable 	The name of a database table for Spark to read data from or write data to.
		driver The class name of the JDBC driver that Spark will instantiate to connect to the 				previous

URL. 		Consult the JDBC driver documentation that you are using. For the MySQL
		Connector/J driver, the class name is com.mysql.jdbc.Driver.

shows an example of reading data from a table called film of the sakila database on a MySQL server that runs on localhost at port 3306.
Listing 4-20. Reading Data from a Table in MySQL Server

// driver for sql server.
val driver = "com.microsoft.sqlserver.jdbc.SQLServerDriver";
val url = "jdbc:sqlserver://wcarroll3:1433;database=mydb;user=ReportUser;password=ReportUser"

val mysqlURL= "jdbc:mysql://localhost:3306/sakila"
val filmDF = spark.read.format("jdbc").option("driver", "com.mysql.jdbc.Driver")
.option("url", mysqlURL)
.option("dbtable", "film")
.option("user", "<username>")
.option("password","<password>")
.load()

filmDF.printSchema
|-- film_id: integer (nullable = false)
|-- title: string (nullable = false)
|-- description: string (nullable = true)
|-- release_year: date (nullable = true)
|-- language_id: integer (nullable = false)
|-- original_language_id: integer (nullable = true)
|-- rental_duration: integer (nullable = false)
|-- rental_rate: decimal(4,2) (nullable = false)
|-- length: integer (nullable = true)
|-- replacement_cost: decimal(5,2) (nullable = false)
|-- rating: string (nullable = true)
|-- special_features: string (nullable = true)
|-- last_update: timestamp (nullable = false)

filmDF.select("film_id","title").show(5)
+-------+-----------------+
|film_id| title|
+-------+-----------------+
| 1| ACADEMY DINOSAUR|
| 2| ACE GOLDFINGER|
| 3| ADAPTATION HOLES|
| 4| AFFAIR PREJUDICE|
| 5| AFRICAN EGG|
+-------+-----------------+

When working with a JDBC data source, Spark pushes the filter conditions all the way down to the RDBMS as much as possible. By doing this, much of the data will be filtered out at the RDBMS level, and therefore this will not only speed up the data filtering logic but dramatically reduce the amount of data Spark needs to read. This optimization technique is known as predicate pushdown, and Spark will often do this when it knows a data source can support the filtering capability. Parquet is another data source that has this capability.

Working with Structured Operations
----------------------------------
Now that you know how to create DataFrames, the next part is to learn how to manipulate or transform them using the provided structured operations. 

Unlike the RDD operations, the structured operations are designed to be more relational, meaning these operations mirror the kind of expressions you can do with SQL, such as projection, filtering, transforming, joining, and so on. 

Similar to RDD operations, the structured operations are divided into two categories: transformation and action. The semantics of the structured transformations and actions are identical to the ones in RDDs. In other words, structured transformations are lazily evaluated, and structured actions are eagerly evaluated.

Structured operations are sometimes described as a domain-specific language (DSL) for distributed data manipulation.

Table in PDF(see for glance) describes the commonly used DataFrame structured transformations. As a reminder, DataFrames are immutable, and their transformation operations always return a new DataFrame.

Working with Columns
--------------------
There are different ways of referring to a column, which has created confusion in the Spark user community. A common question is when to use which one, and the answer is it depends.Below table describes the available options.

Different Ways of Referring to a Column

Way 		Example 			Description
---		-------				-----------
"" 		"columName" 			This refers to a column as a string type.
col 		col("columnName") 		The col function returns an instance of the Column class.
column 		column("columnName") 		Similar to col, this function returns an instance of the Column class.
$ 		$"columnName" 			This is a syntactic sugar way of constructing a Column class in Scala.
'(tick mark)	'columnName 			This is a syntactic sugar way of constructing a Column class in Scala by leveraging the Scala symbolic literals 							feature.

The col and column functions are synonymous, and both are available in the Scala and Python Spark APIs. If you often switch between the Spark Scala and Python APIs,
then it makes sense to use the col function so there is a consistency in your code.

If you mostly or exclusively use the Spark Scala APIs, then my recommendation is to use ' (tick mark) because there is only a single character to type.The DataFrame class has its own col function, which is used to disambiguate between columns with the same name from two or more DataFrames when performing a join.

Different Ways of Referring to a Column

import org.apache.spark.sql.functions._
val kvDF = Seq((1,2),(2,3)).toDF("key","value")

// to display column names in a DataFrame, we can call the columns function
kvDF.columns
Array[String] = Array(key, value)

kvDF.select("key")
kvDF.select(col("key"))

kvDF.select(column("key"))
kvDF.select($"key")
kvDF.select('key)

// using the col function of DataFrame
kvDF.select(kvDF.col("key"))
kvDF.select('key, 'key > 1).show
+---+----------+
|key| (key > 1)|
+---+----------+
| 1| false|
| 2| true|
+---+----------+

The previous example illustrates a column expression, and therefore it is required to specify a column as an instance of the Column class. If the column was specified
as a string, then it would result in a type mismatch error.

Working with Structured Transformations
---------------------------------------
This section provides examples of working with the structured transformations.

Creating the movies DataFrame from a Parquet File

val movies = spark.read.parquet("<path>/chapter4/data/movies/movies.parquet")

select(columns)
---------------
This transformation is commonly used to perform projection, meaning selecting all or a subset of columns from a DataFrame. During the selection, each column can be
transformed via a column expression. There are two variations of this transformation. One takes the column as a string, and the other takes columns as the Column class. This transformation does not permit you to mix the column type when using one of these two variations.

Two Variations of the select Transformation
movies.select("movie_title","produced_year").show(5)
+-------------------+--------------+
| movie_title| produced_year|
+-------------------+--------------+
| Coach Carter| 2005|
| Superman II| 1980|
| Apollo 13| 1995|
| Superman| 1978|
| Back to the Future| 1985|
+-------------------+--------------+
// using a column expression to transform year to decade
movies.select('movie_title,('produced_year - ('produced_year % 10)).as("produced_decade")).show(5)

+-------------------+----------------+
| movie_title		| produced_decade|
+-------------------+----------------+
| Coach Carter		| 2000|
| Superman II 		| 1980|
| Apollo 13		| 1990|
| Superman		| 1970|
| Back to the Future	| 1980|
+-------------------+----------------+


selectExpr(expressions)
-----------------------
This transformation is a variant of the select transformation. The one big difference is that it accepts one or more SQL expressions, rather than columns.

You can express SQL expressions in a string format, and Spark will parse them into a logical tree so they will be evaluated in the right order. Let’s say you want to create a new DataFrame that has all the columns in the movies DataFrame and introduce a new column to represent the decade a movie was produced in; then you would do something like below.

movies.selectExpr("*","(produced_year - (produced_year % 10)) as decade").show(5)
+-----------------+-------------------+--------------+-------+
| actor_name| movie_title| produced_year| decade|
+-----------------+-------------------+--------------+-------+
|McClure, Marc (I)| Coach Carter| 2005| 2000|
|McClure, Marc (I)| Superman II| 1980| 1980|
|McClure, Marc (I)| Apollo 13| 1995| 1990|
|McClure, Marc (I)| Superman| 1978| 1970|
|McClure, Marc (I)| Back to the Future| 1985| 1980|
+-----------------+-------------------+--------------+-------+

The combination of SQL expressions and built-in functions makes it easy to perform certain data analysis that otherwise would take multiple steps. Below shows how easy it is to determine the number of unique movie titles and unique actors in the movies dataset in a single statement. The count function performs an aggregation over the entire DataFrame.

Using a SQL Expression and Built-in Functions

movies.selectExpr("count(distinct(movie_title)) as movies","count(distinct(actor_name)) as actors").show
+-------+-------+
| movies| actors|
+-------+-------+
| 1409	| 6527	|
+-------+-------+

filter(condition), where(condition)
-----------------------------------
This transformation is a fairly straightforward one to understand. It is used to filter out the rows that don’t meet the given condition, in other words, when the condition evaluates to false.

Both the filter and where transformations have the same behavior, so pick the one you are most comfortable with. The latter one is just a bit more relational than the former.

Filter Rows with Logical Comparison Functions in the Column Class

movies.filter('produced_year < 2000)
movies.where('produced_year > 2000)

movies.filter('produced_year >= 2000)
movies.where('produced_year >= 2000)

// equality comparison require 3 equal signs
movies.filter('produced_year === 2000).show(5)
+------------------+---------------------+--------------+
| actor_name| movie_title| produced_year|
+------------------+---------------------+--------------+
| Cooper, Chris (I)| Me, Myself & Irene| 2000|
| Cooper, Chris (I)| The Patriot| 2000|
| Jolie, Angelina| Gone in Sixty Sec...| 2000|
| Yip, Françoise| Romeo Must Die| 2000|
| Danner, Blythe| Meet the Parents| 2000|
+------------------+---------------------+--------------+

// inequality comparison uses an interesting looking operator =!=
movies.select("movie_title","produced_year").filter('produced_year =!=2000).show(5)
+-------------------+--------------+
| movie_title| produced_year|
+-------------------+--------------+
| Coach Carter| 2005|
| Superman II| 1980|
| Apollo 13| 1995|
| Superman| 1978|
| Back to the Future| 1985|
+-------------------+--------------+

// to combine one or more comparison expressions, we will use either the OR
and AND expression operator
movies.filter('produced_year >= 2000 && length('movie_title) < 5).show(5)
+----------------+------------+--------------+
| actor_name| movie_title| produced_year|
+----------------+------------+--------------+
| Jolie, Angelina| Salt| 2010|
| Cueto, Esteban| xXx| 2002|
| Butters, Mike| Saw| 2004|
| Franko, Victor| 21| 2008|
| Ogbonna, Chuk| Salt| 2010|
+----------------+------------+--------------+

// the other way of accomplishing the same result is by calling the filter
function two times
movies.filter('produced_year >= 2000).filter(length('movie_title) < 5).show(5)

distinct, dropDuplicates
------------------------
These two transformations have identical behavior. However, dropDuplicates allows you to control which columns should be used in deduplication logic. If none is specified,the deduplication logic will use all the columns in the DataFrame.

movies.select("movie_title").distinct.selectExpr("count(movie_title) as movies").show
movies.dropDuplicates("movie_title").selectExpr("count(movie_title) as movies").show
+------+
|movies|
+------+
| 1409|
+------+

In terms of performance, there is no difference between these two approaches because Spark will transform them into the same logical plan.

sort(columns), orderBy(columns)
-------------------------------
Both of these transformations have the same semantics. The orderBy transformation is more relational than the other one. By default, the sorting is in ascending order, and it is fairly easy to change it to descending. When specifying more than one column, it is possible to have a different order for each of the columns

Sorting the DataFrame in Ascending and Descending Order

val movieTitles = movies.dropDuplicates("movie_title").selectExpr("movie_title", "length(movie_title) as title_length", , "produced_year")

movieTitles.sort('title_length).show(5)
+-----------+-------------+--------------+
|movie_title| title_length| produced_year|
+-----------+-------------+--------------+
| RV| 2| 2006|
| 12| 2| 2007|
| Up| 2| 2009|
| X2| 2| 2003|
| 21| 2| 2008|
+-----------+-------------+--------------+

// sorting in descending order
movieTitles.orderBy('title_length.desc).show(5)
+---------------------+-------------+--------------+
| movie_title| title_length| produced_year|
+---------------------+-------------+--------------+
| Borat: Cultural L...| 83| 2006|
| The Chronicles of...| 62| 2005|
| Hannah Montana & ...| 57| 2008|
| The Chronicles of...| 56| 2010|
| Istoriya pro Rich...| 56| 1997|
+---------------------+-------------+--------------+

// sorting by two columns in different orders
movieTitles.orderBy('title_length.desc, 'produced_year).show(5)
+---------------------+-------------+--------------+
| movie_title| title_length| produced_year|
+---------------------+-------------+--------------+
| Borat: Cultural L...| 83| 2006|
| The Chronicles of...| 62| 2005|
| Hannah Montana & ...| 57| 2008|
| Istoriya pro Rich...| 56| 1997|
| The Chronicles of...| 56| 2010|
+---------------------+-------------+--------------+
In the previous example, notice the title of the last two movies are at the same length,but their years are ordered in the correct ascending order.

limit(n)
--------
This transformation returns a new DataFrame by taking the first n rows. This transformation is commonly used after the sorting is done to figure out the top n or
bottom n rows based on the sorting order.

// first create a DataFrame with their name and associated length
val actorNameDF = movies.select("actor_name").distinct.selectExpr("*", "length(actor_name) as length")

// order names by length and retrieve the top 10
actorNameDF.orderBy('length.desc).limit(10).show

+-----------------------------+-------+
| actor_name| length|
+-----------------------------+-------+
| Driscoll, Timothy 'TJ' James| 28|
| Badalamenti II, Peter Donald| 28|
| Shepard, Maridean Mansfield| 27|
| Martino, Nicholas Alexander| 27|
| Marshall-Fricker, Charlotte| 27|
| Phillips, Christopher (III)| 27|
| Pahlavi, Shah Mohammad Reza| 27|
| Juan, The Bishop Don Magic| 26|
| Van de Kamp Buchanan, Ryan| 26|
| Lough Haggquist, Catherine| 26|
+-----------------------------+-------+

union(otherDataFrame)
----------------------
You learned earlier that DataFrames are immutable. So if there is a need to add more rows to an existing DataFrame, then the union transformation is useful for that purpose as well as for combining rows from two DataFrames. This transformation requires both DataFrames to have the same schema, meaning both column names and their
order must exactly match.

Let say one of the movies in the DataFrame is missing an actor and you want to fix that issue.

Adding a Missing Actor to the movies DataFrame

// we want to add a missing actor to movie with title as "12"
val shortNameMovieDF = movies.where('movie_title === "12")
shortNameMovieDF.show
+---------------------+------------+---------------+
| actor_name| movie_title| produced_year |
+---------------------+------------+---------------+
| Efremov, Mikhail| 12| 2007|
| Stoyanov, Yuriy| 12| 2007|
| Gazarov, Sergey| 12| 2007|
| Verzhbitskiy, Viktor| 12| 2007|
+---------------------+------------+---------------+

// create a DataFrame with one row
import org.apache.spark.sql.Row
val forgottenActor = Seq(Row("Brychta, Edita", "12", 2007L))
val forgottenActorRDD = spark.sparkContext.parallelize(forgottenActor)
val forgottenActorDF = spark.createDataFrame(forgottenActorRDD,shortNameMovieDF.schema)

// now adding the missing action
val completeShortNameMovieDF = shortNameMovieDF.union(forgottenActorDF)
completeShortNameMovieDF.union(forgottenActorDF).show
+---------------------+------------+--------------+
| actor_name| movie_title| produced_year|
+---------------------+------------+--------------+
| Efremov, Mikhail| 12| 2007|
| Stoyanov, Yuriy| 12| 2007|
| Gazarov, Sergey| 12| 2007|
| Verzhbitskiy, Viktor| 12| 2007|
| Brychta, Edita| 12| 2007|
+---------------------+------------+--------------+

withColumn(colName, column)
----------------------------
This transformation is used to add a new column to a DataFrame. It requires two input parameters: a column name and a value in the form of a column expression. You
can accomplish pretty much the same goal by using the selectExpr transformation. However, if the given column name matches one of the existing ones, then that column
is replaced with the given column expression.

// adding a new column based on a certain column expression
movies.withColumn("decade", ('produced_year - 'produced_year % 10)).show(5)
+------------------+-------------------+--------------+-------+
| actor_name| movie_title| produced_year| decade|
+------------------+-------------------+--------------+-------+
| McClure, Marc (I)| Coach Carter| 2005| 2000|
| McClure, Marc (I)| Superman II| 1980| 1980|
| McClure, Marc (I)| Apollo 13| 1995| 1990|
| McClure, Marc (I)| Superman| 1978| 1970|
| McClure, Marc (I)| Back to the Future| 1985| 1980|
+------------------+-------------------+--------------+-------+

// now replace the produced_year with new values
movies.withColumn("produced_year", ('produced_year - 'produced_year % 10)).show(5)
+------------------+-------------------+--------------+
| actor_name| movie_title| produced_year|
+------------------+-------------------+--------------+
| McClure, Marc (I)| Coach Carter| 2000|
| McClure, Marc (I)| Superman II| 1980|
| McClure, Marc (I)| Apollo 13| 1990|
| McClure, Marc (I)| Superman| 1970|
| McClure, Marc (I)| Back to the Future| 1980|
+------------------+-------------------+--------------+

withColumnRenamed(existingColName, newColName)
----------------------------------------------
This transformation is strictly about renaming an existing column name in a DataFrame.this transformation is useful in the following 2 situations:

• To rename a cryptic column name to a more human-friendly name. The cryptic column name can come from an existing schema that you don’t have control of, such as when the column you need in a Parquet file was produced by your company’s partner.

• Before joining two DataFrames that happen to have one or more same column name. This transformation can be used to rename one or more columns in one of the two DataFrames so you can refer to them easily after the join.

Notice that if the provided existingColName doesn’t exist in the schema, Spark doesn’t throw an error, and it will silently do nothing.By the way, this is something
that can be accomplished by using the select or selectExpr transformation.

movies.withColumnRenamed("actor_name", "actor")
.withColumnRenamed("movie_title", "title")
.withColumnRenamed("produced_year", "year").show(5)
+------------------+-------------------+-----+
| actor| title| year|
+------------------+-------------------+-----+
| McClure, Marc (I)| Coach Carter| 2005|
| McClure, Marc (I)| Superman II| 1980|
| McClure, Marc (I)| Apollo 13| 1995|
| McClure, Marc (I)| Superman| 1978|
| McClure, Marc (I)| Back to the Future| 1985|
+------------------+-------------------+-----+

drop(columnName1, columnName2)
------------------------------
This transformation simply drops the specified columns from the DataFrame. You can specify one or more column names to drop, but only the ones that exist in the schema
will be dropped and the ones that don’t will be silently ignored. You can use the select transformation to drop columns by projecting only the columns that you want to keep. In the case that a DataFrame has 100 columns and you want to drop a few, then this transformation is more convenient to use than the select transformation.

movies.drop("actor_name", "me").printSchema
|-- movie_title: string (nullable = true)
|-- produced_year: long (nullable = true)

As you can see from the previous example, the second column, me, doesn’t exist in the schema, so the drop transformation simply ignores it.

sample(fraction), sample(fraction, seed), sample(fraction, seed,withReplacement)
---------------------------------------------------------------------------------
This transformation returns a randomly selected set of rows from the DataFrame. The number of the returned rows will be approximately equal to the specified fraction, which represents a percentage, and the value has to be between 0 and 1. The seed is used to seed the random number generator, which is used to generate a row number to include in the result. If a seed is not specified, then a randomly generated value is used. The withReplacement option is used to determine whether a randomly selected row will be placed back into the selection pool. In other words, when withReplacement is true, a particular selected row has the potential to be selected more than once. So, when would you need to use this transformation? It is useful in the case where the original dataset is large and there is a need to reduce it to a smaller size so you can quickly iterate on the data analysis logic.

// sample with no replacement and a fraction
movies.sample(false, 0.0003).show(3)
+---------------------+----------------------+--------------+
| actor_name| movie_title| produced_year|
+---------------------+----------------------+--------------+
| Lewis, Clea (I)| Ice Age: The Melt...| 2006|
| Lohan, Lindsay| Herbie Fully Loaded| 2005|
| Tagawa, Cary-Hiro...| Licence to Kill| 1989|
+---------------------+----------------------+--------------+

// sample with replacement, a fraction and a seed
movies.sample(true, 0.0003, 123456).show(3)
+---------------------+---------------+--------------+
| actor_name| movie_title| produced_year|
+---------------------+---------------+--------------+
| Panzarella, Russ (V)| Public Enemies| 2009|
| Reed, Tanoai| Daredevil| 2003|
| Moyo, Masasa| Spider-Man 3| 2007|
+---------------------+---------------+--------------+

randomSplit(weights)
--------------------
This transformation is commonly used during the process of preparing the data to train machine learning models. Unlike the previous transformations, this one returns one or more DataFrames. The number of DataFrames it returns is based on the number of weights you specify. If the provided set of weights don’t add up to 1, then they will be normalized accordingly to add up to 1.

// the weights need to be an Array
val smallerMovieDFs = movies.randomSplit(Array(0.6, 0.3, 0.1))

// let's see if the counts are added up to the count of movies DataFrame
movies.count
Long = 31393

smallerMovieDFs(0).count
Long = 18881

smallerMovieDFs(0).count + smallerMovieDFs(1).count + smallerMovieDFs(2).count
Long = 31393

Working with Missing or Bad Data
--------------------------------
The Spark community recognizes that the need to deal with missing data is a fact of life; therefore, Spark provides a dedicated class called DataFrameNaFunctions to help in dealing with this inconvenient issue. An instance of DataFrameNaFunctions is available as the an member variable inside the DataFrame class. There are three common ways of dealing with missing or bad data. 

The first way is to drop the rows that have missing values in a one or more columns. 
The second way is to fill those missing values with user-provided values. 
The third way is to replace the bad data with something that you know how to deal with.

// dropping entire rows that have missing data in any single column
// both of the lines below will achieve the same purpose
badMoviesDF.na.drop().show
badMoviesDF.na.drop("any").show
+----------+------------+--------------+
|actor_name| movie_title| produced_year|
+----------+------------+--------------+
+----------+------------+--------------+

// drop rows that have missing data in every single(all) column/drop entire row if all columns in a row having null value.
badMoviesDF.na.drop("all").show
+----------+--------------+--------------+
|actor_name| movie_title| produced_year|
+----------+--------------+--------------+
| null| null| 2018|
| John Doe| Awesome Movie| null|
| null| Awesome Movie| 2018|
| Mary Jane| null| 2018|
+----------+--------------+--------------+

// drops rows when column actor_name has missing data
badMoviesDF.na.drop(Array("actor_name")).show
+----------+--------------+--------------+
|actor_name| movie_title| produced_year|
+----------+--------------+--------------+
| John Doe| Awesome Movie| null|
| Mary Jane| null| 2018|
+----------+--------------+--------------+


describe(columnNames)
---------------------
Sometimes it is useful to have a general sense of the basic statistics of the data you are working with. The basic statistics this transformation can compute for string and numeric columns are count, mean, standard deviation, minimum, and maximum. You can pick and choose which string or numeric columns to compute the statistics for.

movies.describe("produced_year").show
+-------+-------------------+
|summary| produced_year|
+-------+-------------------+
| count| 31392|
| mean| 2002.7964449541284|
| stddev| 6.377236851493877|
| min| 1961|
| max| 2012|
+-------+-------------------+

Working with Structured Actions
-------------------------------
This section covers the structured actions. They have the same eager evaluated semantics as the RDD actions, so they trigger the computation of all the transformations
that lead up to a particular action.

Operation Description
show(),show(numRows),show(truncate),show(numRows, truncate)
----------------------------------------------------------
Displays a number of rows in a tabular format. If numRows is not specified, it will show the top 20 rows. The truncate option controls whether to truncate the string column if it is longer than 20 characters.

head(),first(),head(n),take(n)
------------------------------
Returns the first row. If n is specified, then it will return the first n rows. first is an alias for head. take(n) is an alias for first(n). 

takeAsList(n)
-------------
Returns the first n rows as a Java list. Be careful not to take too many rows; otherwise, it may cause an out-of-memory error on the application’s driver process.

collect, collectAsList
-----------------------
Returns all the rows as an array or Java list. Apply the same caution as the one described in the takeAsList action. count Returns the number of rows in a DataFrame.

Introduction to Datasets
------------------------
At one point in the history of Spark, there was a lot of confusion about the differences between the DataFrame and Dataset APIs.

Starting with the Spark 2.0 release, there is only one high-level abstraction called a Dataset, which has two flavors: a strongly typed API and an untyped API. The term DataFrame didn’t go away; instead, it has been redefined as an alias for a collection of generic objects in a Dataset.

what I am saying is a DataFrame is essentially a type alias for Dataset[Row], where a Row is a generic untyped JVM object. A Dataset is defined as a collection of strongly typed JVM objects, represented by either a case class in Scala or a class in Java.

Below table describes the Dataset API flavors that are available in each of the programming languages that Spark supports.

The Python and R languages have no compile-time type safety; therefore, only the untyped Dataset APIs (aka DataFrame) are supported.

Dataset Flavors
---------------
Language	Flavor
-----------------------
Scala 		Dataset[T] and DataFrame
Java 		Dataset[T]
Python 		DataFrame
R 		DataFrame

Consider the Dataset as a younger brother of the DataFrame; however, it is more about type safety and is object-oriented. A Dataset is a strongly typed, immutable collection of data. Similar to a DataFrame, the data in a Dataset is mapped to a defined schema.

However, there are a few important differences between a DataFrame and a Dataset.

• Each row in a Dataset is represented by a user-defined object so that you can refer to an individual column as a member variable of that object. This provides you with compile-type safety.

• A Dataset has helpers called encoders, which are smart and efficient encoding utilities that convert data inside each user-defined object into a compact binary format. This translates into a reduction of memory usage if and when a Dataset is cached in memory as well as a reduction in the number of bytes that Spark needs to transfer over a network during the shuffling process.

In terms of limitations, the Dataset APIs are available in only strongly typed languages such as Scala and Java. At this point, a question should pop into your mind
regarding when to use the DataFrame APIs and the Dataset APIs. The Dataset APIs are good for production jobs that need to run on a regular basis and are written and
maintained by data engineers. For most interactive and explorative analysis use cases, using the DataFrame APIs would be sufficient.

Creating Datasets
-----------------
There are a few ways to create a Dataset, but the first thing you need to do is to define a domain-specific object to represent each row. The first way is to transform a DataFrame to a Dataset using the as(Symbol) function of the DataFrame class. The second way is to use the SparkSession.createDataset() function to create a Dataset from a local collection objects. The third way is to use the toDS implicit conversion utility. Below examples of creating Datasets using the different ways described earlier.

Different Ways of Creating Datasets

// define Movie case class
case class Movie(actor_name:String, movie_title:String, produced_year:Long)

// convert DataFrame to strongly typed Dataset
val moviesDS = movies.as[Movie]

// create a Dataset using SparkSession.createDataset() and the toDS
implicit function
val localMovies = Seq(Movie("John Doe", "Awesome Movie", 2018L),
Movie("Mary Jane", "Awesome Movie", 2018L))

val localMoviesDS1 = spark.createDataset(localMovies)
val localMoviesDS2 = localMovies.toDS()
localMoviesDS1.show

+----------+--------------+--------------+
|actor_name| movie_title| produced_year|
+----------+--------------+--------------+
| John Doe | Awesome Movie| 2018|
| Mary Jane| Awesome Movie| 2018|
+----------+--------------+--------------+

Out of the three ways of creating Datasets, the first way is the most popular one. During the process of transforming a DataFrame to a Dataset using a Scala case class, Spark will perform a validation to ensure the member variable names in the Scala case class matches up with the column names in the schema of the DataFrame. If there is a mismatch, Spark will let you know.

Working with Datasets
----------------------
Now that you have a Dataset, you can manipulate it using the transformations and actions described earlier. Previously you referred to the columns in the DataFrame
using one of the options described earlier. With a Dataset, each row is represented by a strongly typed object; therefore, you can just refer to the columns using the member variable names, which will give you type safety as well as compile-time validation. If there is a misspelling in the name, the compiler will flag them immediately during the development phase.

Manipulating a Dataset in a Type-Safe Manner

// filter movies that were produced in 2010 using
moviesDS.filter(movie => movie.produced_year == 2010).show(5)
+-------------------+---------------------+--------------+
| actor_name		| movie_title	| produced_year|
+-------------------+---------------------+--------------+
| Cooper, Chris (I)	| The Town	| 2010|
| Jolie, Angelina	| Salt		| 2010|
| Jolie, Angelina	| The Tourist	| 2010|
| Danner, Blythe	| Little Fockers| 2010|
| Byrne, Michael (I)	| Harry Potter and ...| 2010|
+-------------------+---------------------+--------------+

// displaying the title of the first movie in the moviesDS
moviesDS.first.movie_title
String = Coach Carter

// try with misspelling the movie_title and get compilation error
moviesDS.first.movie_tile
error: value movie_tile is not a member of Movie

// perform projection using map transformation
val titleYearDS = moviesDS.map(m => ( m.movie_title, m.produced_year))
titleYearDS.printSchema
|-- _1: string (nullable = true)
|-- _2: long (nullable = false)

// demonstrating a type-safe transformation that fails at compile time,performing subtraction on a column with string type
// a problem is not detected for DataFrame until runtime
movies.select('movie_title - 'movie_title)

// a problem is detected at compile time
moviesDS.map(m => m.movie_title - m.movie_title)
error: value - is not a member of String

// take action returns rows as Movie objects to the driver

moviesDS.take(5)
Array[Movie] = Array(Movie(McClure, Marc (I),Coach Carter,2005),Movie(McClure, Marc (I),Superman II,1980), Movie(McClure, Marc (I),Apollo13,1995))

When you use the strongly typed Dataset APIs, Spark implicitly converts each Row instance to the domain-specific object that you provide. This conversion has some cost
in terms of performance; however, it provides more flexibility.

One general guideline to help with deciding when to use a DataSet over a DataFrame is the desire to have a higher degree of type safety at compile time.

Using SQL in Spark SQL
----------------------
One of the coolest features Spark provides is the ability to use SQL to perform distributed data manipulation or analysis at scale. Data analysts who are proficient at SQL can now be productive at using Spark to perform data analysis on large datasets.

One important note to remember is SQL in Spark is designed to be used for online analytic processing (OLAP) use cases and not online transaction processing (OLTP) use cases. In other words, it is not applicable for low-latency use cases.

Running SQL in Spark
--------------------
Spark provides a few different ways to run SQL in Spark.
• Spark SQL CLI (./bin/spark-sql)
• JDBC/ODBC server
• Programmatically in Spark applications

This first two options provide an integration with Apache Hive to leverage the Hive metastore, which is a repository that contains the metadata and schema information
about the various system and user-defined tables. This section will cover only the last option.

DataFrames and Datasets are essentially like tables in a database. Before you can issue SQL queries to manipulate them, you need to register them as temporary views.
Each view has a name, and that is what is used as the table name in the select clause.

Spark provides two levels of scoping for the temporary views. 

One is at the Spark session level. When a DataFrame is registered at this level, only the queries that are issued in the same session can refer to that DataFrame. The session-scoped level will disappear when a Spark session is closed.

The second scoping level is at the global level, which means these views are available to SQL statements in all Spark sessions. All the registered views are maintained in the Spark metadata catalog that can be accessed through SparkSession.

Registering the movies DataFrame as a Temporary View and Inspecting the Spark Metadata Catalog

// display tables in the catalog, expecting an empty list
spark.catalog.listTables.show
+-----+---------+------------+----------+------------+
| name| database| description| tableType| isTemporary|
+-----+---------+------------+----------+------------+

// now register movies DataFrame as a temporary view
movies.createOrReplaceTempView("movies")

// should see the movies view in the catalog
spark.catalog.listTables.show
+-------+---------+------------+----------+------------+
| name| database| description| tableType| isTemporary|
+-------+---------+------------+----------+------------+
| movies| null| null| TEMPORARY| true|
+-------+---------+------------+----------+------------+

// show the list of columns of movies view in catalog
spark.catalog.listColumns("movies").show
+--------------+------------+---------+---------+------------+---------+
| name		| description| dataType| nullable| isPartition| isBucket|
+--------------+------------+---------+---------+------------+---------+
| actor_name	| null| string| true| false| false|
| movie_title	| null| string| true| false| false|
| produced_year	| null| bigint| true| false| false|
+--------------+------------+---------+---------+------------+---------+

// register movies as global temporary view called movies_g
movies.createOrReplaceGlobalTempView("movies_g")

The programmatic way of issuing SQL queries is to use the sql function of the SparkSession class. Inside the SQL statement, you have access to all SQL expressions and built-in functions. Once the SparkSession.sql function executes the given SQL query, it will return a DataFrame.

Programmatically Executing SQL Statements in Spark

// simple example of executing a SQL statement without a registered view 
val infoDF = spark.sql("select current_date() as today , 1 + 100 as value")
infoDF.show
+----------+------+
| today| value|
+----------+------+
|2017-12-27| 101|
+----------+------+

// select from a view
spark.sql("select * from movies where actor_name like '%Jolie%' and produced_year > 2009").show
+---------------+----------------+--------------+
| actor_name| movie_title| produced_year|
+---------------+----------------+--------------+
|Jolie, Angelina| Salt			| 2010|
|Jolie, Angelina| Kung Fu Panda 2	| 2011|
|Jolie, Angelina| The Tourist		| 2010|
+---------------+----------------+--------------+

// mixing SQL statement and DataFrame transformation
spark.sql("select actor_name, count(*) as count from movies group by actor_name")
.where('count > 30)
.orderBy('count.desc)
.show
+-------------------+------+
| actor_name		| count|
+-------------------+------+
| Tatasciore, Fred	| 38|
| Welker, Frank		| 38|
| Jackson, Samuel L.	| 32|
| Harnell, Jess		| 31|
+-------------------+------+

// using a subquery to figure out the number movies were produced in each year.
// leverage """ to format multi-line SQL statement
spark.sql("""select produced_year, count(*) as count from (select distinct movie_title, produced_year from movies) group by produced_year""")
.orderBy('count.desc).show(5)
+-------------+------+
|produced_year| count|
+-------------+------+
| 2006| 86|
| 2004| 86|
| 2011| 86|
| 2005| 85|
| 2008| 82|
+-------------+------+

// select from a global view requires prefixing the view name with key word 'global_temp'
spark.sql("select count(*) from global_temp.movies_g").show
+-----+
|count|
+-----+
|31393|
+-----+

Instead of reading the data file through DataFrameReader and then registering the newly created DataFrame as a temporary view, it is possible to issue a SQL query directly from a file

spark.sql("SELECT * FROM parquet.`<path>/chapter4/data/movies/movies.parquet`").show(5)

Writing Data Out to Storage Systems
-----------------------------------
You will need to write the data in a DataFrame out to an external storage system, i.e., a local file system, HDFS,or Amazon S3. In a typical ETL data processing job, the results will most likely be written out to some storage system.

In Spark SQL, the DataFrameWriter class is responsible for the logic and complexity of writing out the data in a DataFrame to an external storage system. An instance of the DataFrameWriter class is available to you as the write variable in the DataFrame class.

movies.write

Common Interacting Pattern with DataFrameWriter
movies.write.format(...).mode(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save(path)

Similar to DataFrameReader, the default format is Parquet; therefore, it is not necessary to specify a format when writing the data out in Parquet format.

The partitionBy, bucketBy, and sortBy functions are used to control the directory structure of the output files in the file-based data sources.

One of the important options in the DataFrameWriter class is the save mode, which controls how Spark will handle the situation when the specified output folder already
exists.

Save Modes

Mode 		Description
-------------------------------------------------------
append 		This appends the DataFrame data to the list of files that already exist at
		the specified destination location.

overwrite 	This completely overwrites any data files that already exist at the
		specified destination location with the data in the DataFrame.

error,errorIfExists,default
This is the default mode. If the specified destination location exists, then
DataFrameWriter will throw an error.

ignore 		If the specified destination location exists, then simply do nothing. In other
		words, silently don’t write out the data in the DataFrame.

.Using DataFrameWriter to Write Data to File-Based Sources

// write data out as CVS format, but using a '#' as delimiter
movies.write.format("csv").option("sep", "#").save("/tmp/output/csv")

// write data out using overwrite save mode
movies.write.format("csv").mode("overwrite").option("sep", "#").save("/tmp/output/csv")

The number of files written out to the output directory corresponds to the number of partitions a DataFrame has.

Displaying the Number of Partitions a DataFrame Has

movies.rdd.getNumPartitions
Int = 1

In some cases, the content of a DataFrame is not large, and there is a need to write to a single file. A small trick to achieve this goal is to reduce the number of partitions in your DataFrame to one and then write it out.

Reducing the Number of Partitions in a DataFrame to 1

val singlePartitionDF = movies.coalesce(1)

The idea of writing data out using partitioning and bucketing is borrowed from the Apache Hive user community.

Let’s say you are going to write out the movies DataFrame with partitioning by the produced_year column. DataFrameWriter will write out all the movies with the same produced_year into a single directory. The number of directories in the output folder will correspond to the number of years in the movies DataFrame.

Writing the movies DataFrame Using the Parquet Format and Partition by the produced_year Column

movies.write.partitionBy("produced_year").save("/tmp/output/movies")

// the /tmp/output/movies directory will contain the following subdirectories
produced_year=1961 to produced_year=2012

The Trio: DataFrames, Datasets, and SQL
----------------------------------------
Now you know there are three different ways of manipulating structured data in the Spark SQL module. Below shows where each option falls in the syntax and analysis
spectrum.

			SQL 		DataFrame 	Dataset
--------------------------------------------------------------------
System errors 		Runtime 	Compile time 	Compile time
Analysis errors 	Runtime 	Runtime 	Compile time

DataFrame Persistence
---------------------
DataFrames can be persisted/cached in memory just like how it is done with RDDs.However, there is one big difference when caching a DataFrame. Spark SQL knows
the schema of the data inside a DataFrame, so it organizes the data in a columnar format as well as applies any applicable compressions to minimize space usage.
The net result is it will require much less space to store a DataFrame in memory than storing an RDD when both are backed by the same data file.

Persisting a DataFrame with a Human-Readable Name
val numDF = spark.range(1000).toDF("id")

// register as a view
numDF.createOrReplaceTempView("num_df")

// use Spark catalog to cache the numDF using name "num_df"
spark.catalog.cacheTable("num_df")

// force the persistence to happen by taking the count action
numDF.count